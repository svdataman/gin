# -----------------------------------------------------------
# define power spectrum model in terms of parameters 'theta'

psd.model <- function(theta, f) {
  
  # -----------------------------------------------------------
  # psd.model
  # Inputs: 
  #   theta - vector of (hyper-)parameters for PSD
  #   f     - frequencies at which to compute PSD
  #
  # Value:
  #  psd   - power density at frequencies f
  #
  # Description:
  #  Compute the Power Spectral Density (PSD) at input frequencies
  # based on parameters theta. 
  #
  # History:
  #  16/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------
  
  # check arguments
  if (missing(theta)) {stop('** Missing THETA argument in PSD.MODEL')}
  if (missing(f)) {stop('** Missing F argument in PSD.MODEL')}
  
  # PSD is symmetric   
  f <- abs(f)
  df <- f[2]-f[1]
  
  # parameters 
  a <- theta[2]
  n <- 10^theta[1] 
  
  f.min <- 1e-2
  
  # define power law model  
  psd <- n * f^(-a)
  mask <- (f < f.min)
  min.j <- which.min(f[f > f.min])
  psd[mask] <- (psd[f > f.min])[min.j]
  
  # ensure DC power = 0  
#  ii <- which.min(f)
#  if (f[ii] == 0) { psd[ii] <- 0 }
  
  # finished
  return(psd)
}

# -----------------------------------------------------------
# compute ACF from power spectral model

acf.model <- function(theta, lags, diagnose=FALSE) {
  
  # -----------------------------------------------------------
  # acf.model
  # Inputs: 
  #   theta - vector of (hyper-)parameters for ACF/PSD
  #   lags  - lags at which to compute ACF
  #
  # Value:
  #  acov   - Auto-covariance ACF(lags)
  #
  # Description:
  #  Compute the auto-covariance function (ACF) at input lags
  # based on parameters theta. In practice the ACF is the Fourier
  # transform of a PSD model. First we define the Fourier frequencies,
  # then compute the PSD model at these frequencies, then we include
  # the negatives frequencies - PSD(-f) = PSD(f) - and perform an FFT. 
  # This gives the ACF values. We keep only the ACF values at zero and 
  # positive lags
  #
  # History:
  #  16/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------
  
  # check arguments
  if (missing(theta)) {stop('** Missing THETA argument in ACF.MODEL')}
  if (missing(lags)) {stop('** Missing LAGS argument in ACF.MODEL')}
  
#   # compute corresponding frequencies
#   # Note: we need to use twice the usual Fourier frequencies here
#   n.t <- length(lags)
#   duration <- lags[n.t]
#   
#   df <- 1/duration/2
#   nf <- n.t          
#   f.max <- nf * df
#   f <- seq(0, f.max, by=df)  
#   
#   # compute PSD model
#   psd <- psd.model(theta, f)
#   
#   # make sure the input PSD is defined for equally spaced positive 
#   # frequencies 'f' running 0,1,...,N.f.
#   n.f <- length(psd)
#   
#   # include the negative frequencies (PSD is symmetric)
#   # factor of 1/2 needed when we move from one-sided to
#   # two-sided PSD
#   mask <- (1:(n.f-2))+1
#   psd.sym <- c(psd, rev(psd[mask])) / 2
#   
#   # compute the FFT
#   # XXX check normalisation term XXX
#   dt <- 1/f.max/2
#   acov <- Re(fft(psd.sym)) / length(psd.sym) / dt
#   
#   # keep only the positive lags (since symmetric)
#   acov <- acov[1:(n.f-1)]
#   
#   if (diagnose == TRUE) {
#     cat('-- theta: ', theta, fill=TRUE)
#     cat('-- lags:  ', lags, fill=TRUE)
#     cat('-- acf:   ', acov, fill=TRUE)
#   }
  
#  plot(psd)
#  plot(acov)
  
  acov <- (10^theta[1])*exp(-lags/theta[2])
  
  return(acov)
  
}

# -----------------------------------------------------------
# build the covariance matrix from the PSD model
#
#  tau = [N.i, N.j] matrix of lags tau[i,j] = |t.i - t.j|

acv <- function(theta, tau) {
  
  # -----------------------------------------------------------
  # matrix.acf
  # Inputs: 
  #   theta - vector of (hyper-)parameters for ACF/PSD
  #   tau   - N*M array of lags at which to compute ACF
  #
  # Value:
  #  acf.ij - array of auto-covariances ACF(tau)
  #
  # Description:
  #  Compute the auto-covariance function (ACF) at input lags tau,
  # based on parameters theta. Tau is an N*M array of lags. We first
  # compute the 1-dimensional ACF(lags) and then use linear interpolation
  # to compute the ACF at every lag of the array tau[i,j] = |t1[j] - t2[i]|
  #
  # Note: we cannot simply assume tau is a cyclic, or square matrix,
  # hence we need to compute the ACF at each tau value explicitly.
  #
  # History:
  #  16/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------

  # use package Matrix for Matrix classes and functions
  require(Matrix)
  
  # check arguments
  if (missing(theta)) {stop('** Missing THETA argument in MATRIX.ACF')}
  if (missing(tau)) {stop('** Missing TAU argument in MATRIX.ACF')}
  if (length(dim(tau)) != 2) {stop('** TAU is not 2d array in MATRIX.ACF')}
  
  # ranges for computing ACF
  t.range <- range(tau[tau > 0])
  dt.min <- t.range[1]
  dt.max <- t.range[2]
  n.i <- NROW(tau)
  n.j <- NCOL(tau)
  
  # vector of time delays for computing 1-dimensional ACF(dt)
  # XXX check this XXX
  n.step <- max(n.i, n.j, dt.max/dt.min+1)
  n.step <- n.step - n.step%%2             #  make even length
  lags <- seq(0, dt.max, length.out=n.step)
  
  # compute 1-dimensional ACF
  acf.i <- acf.model(theta, lags)
  
  # now interpolate this to give ACF at tau_ij
  # and reshape into an N.i*N.j matrix
  acf.func <- approxfun(lags, acf.i, yright=0)
  acf.ij <- acf.func(tau)
  dim(acf.ij) <- c(n.i, n.j)
  acf.ij <- Matrix(acf.ij)
  
  # all done
  return(acf.ij)
  
}

# -----------------------------------------------------------

loglike <- function(theta, tau=NULL, dat, PDcheck=FALSE) {
  
  # -----------------------------------------------------------
  # loglike
  # Inputs: 
  #   theta - vector of parameters for covariance function
  #            the first element is the mean value mu
  #   tau   - N*N array of lags at which to compute ACF
  #   dat   - an n * 3 data frame/array, 3 columns
  #            give the times, measurement and errors of
  #            the n data points.
  #
  # Value:
  #  logl - value of log[likelihood(theta)]
  #
  # Description:
  #  Compute the log-likelihood for model parameters theta given data {t, y, dy}
  #  and an (optional) n*N matrix of lags, tau. See algorithm 2.1 of Rasmussen &
  #  Williams (2006). The input data frame 'dat' should contain three columns:
  #  t, y, dy. t[i] and y[i] give the times and the measured values of those
  #  times. dy gives the 'error' on the measurements y, assumed to be
  #  independent Gaussian errors wih standard deviation dy. If dy is not present
  #  we assumine dy[i] = 0 for all i. The columns t, y, and dy are all n-element
  #  vectors.
  #  
  #  For multivariate normal distribution the likelihood is
  #  
  #  L(theta) = (2*pi)^(-n/2) * det(C)^(-1/2) * exp(-1/2 *
  #  (y-mu)^T.C^(-1).(y-mu))
  #  
  #  where y is an n-element vector of data and C is an n*n covariance matrix
  #  (positive, symmetric, semi-definite). We compute l = log(L) which can be
  #  written
  #  
  #    l = -(n/2) * log(2*pi) 
  #        - (1/2)*log(det(C)) 
  #        - (1/2) * (y-mu)^T.C^(-1).(y-mu)
  #  
  #  The n*n matrix inverse C^(-1) is slow. Cholesky decomposition allows a
  #  faster calculation of l:
  #  
  #    C = L.L^T 
  #    det(C) = prod( L[i,i]^2 ) 
  #    log(det(C)) = 2 * sum( log(L[i,i])) = 2 * sum(diag(L))
  #
  #  and 
  #
  #    C^(-1) = (L L^T)^(-1) = (L^-1)^T (L^-1)
  #    Q = (y-mu)^T.C^(-1).(y-mu)
  #      = (y')^T.(L^-1)^T.(L^-1).(y')
  #      = [(L^-1).(y')]^T.[(L^-1).(y')]
  #      = z^T.z
  #   where z = L^(-1).y' --> y' = L.z --> z = solve(L,y')
  #   and y' = y - mu
  # 
  # History:
  #  16/12/13 - v0.1 - First working version
  #  04/07/16 - v0.2 - Major ewrite
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------

  # check arguments
  if (missing(theta)) {stop('** Missing theta input.')}
  if (!all(is.finite(theta))) {stop('** Non-finite values in theta.')}
  if (missing(dat)) {stop('** Missing dat input')}
  if (missing(dat$y)) {stop('** Missing dat$y.')}
  
  # length of data vector(s)  
  n <- length(dat$y)
  
  # if there are no errors, and the dat$dy column is
  # missing, make a column of zeroes.
  if (missing(dat$dy)) { 
    dat$dy <- array(0, n) 
  }
  
  # if n * n array tau is not present then make one
  if (is.null(tau)) {
    tau <- abs( outer(dat$t, dat$t, "-") )
  }
  
  # make sure y and tau have matching lengths  
  if (ncol(tau) != n) {
    stop('** y and tau do not have matching dimensions') 
  }
  
  # first, extract the mean (mu) from the parameter vector theta,
  # the extract the error scaling parameter (nu) from the vector theta,
  # Then remove these the theta, so now these only contains parameter for
  # the ACV function
  mu <- theta[1]
  nu <- theta[2]
  theta <- theta[c(-1,-2)]
  
  # now subtract the mean from the data: y <- (y - mu)
  y <- dat$y - mu
  
  # compute the covariance matrix C as C[i,j] = ACV(tau[i,j])
  # using the remaining parameters
  C <- acv(theta, tau)
  
  # check there aren't any missing values
  if (!all(is.finite(cov))) {
    cat('Non-finite values in model covariance matrix.')
    return(NULL)
  }
  
  # add the error matrix diag(dy^2) 
  diag(C) <- diag(C) + nu*dy*dy
  
  # enforce for positive-definiteness (PD) and symmetry
  # The covariance matrix should be a PD matrix but numerical
  # errors may mean this is not exactly true. We use the 
  # nearPD function from the Matrix package to find the nearest
  # PD matrix and use his.
  if (PDcheck == TRUE) {
    pd <- Matrix::nearPD(C)
    if (pd$converged == FALSE) {
      warning('** nearPD failed to converge.')
    }
    C <- pd$mat
  }
  
  # compute the easy (constant) part of the log likelihood.
  l.1 <- -(n/2) * log(2*pi)
  
  # find the Cholesky decomposition (lower left)
  L <- t( chol(C) )
  
  # first, compute the log|C| term
  l.2 <- -sum( log( diag(L) ) )
  
  # then compute the quadratic form -(1/2) * (y-mu)^T C^-1 (y-mu)
  z <- solve(L, y)
  l.3 <- -0.5 * (z %*% z)[1]
  
  # combine all three terms for give the log[likelihood]
  loglike <- l.1 + l.2 + l.3
  return(loglike) 
}

# -----------------------------------------------------------
# Build the matrix of lags tau[i,j] = |t.i - t.j|

matrix.tau <- function(t.i, t.j) {
  
  # -----------------------------------------------------------
  # matrix.tau
  # Inputs: 
  #   t.i   - times t.1 for vector y[t.1[i]]
  #   t.j   - times t.2 for vector y[t.2[j]]
  #
  # Value:
  #  tau.ij - matrix of time lags 
  #
  # Description:
  #  Define the matrix of time lags
  #   tau[i,j] = |t.1[i] - t.2[j]|
  #
  # Note that in the special case that t.1=t.2 we have
  # a square symmetric matrix: tau[i,j] = tau[j,i]. 
  # In the even more special case that
  # the two series are identically and evenly sampled
  # (t.1[i] = t.2[i] = i * dt + t0) then we have a 
  # circulant matrix; the jth column tau[,j] is the (j-1)th 
  # cyclic permutation of the first column. This matrix is
  # symmetric, Toeplitz and circulant. 
  #
  # History:
  #  16/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------
  
  # check arguments
  if (missing(t.i)) { stop('** Missing t.i array in MATRIX.TAU.')}
  if (missing(t.j)) { t.j <- t.i }

  # compute t.0, an arbitrary start time 
  t.0 <- min(t.i, t.j)

  # compute the time lag matrix
  tau <- abs(outer(t.i-t.0, t.j-t.0, "-"))

  # return to calling function
  return(tau)  
}

# -----------------------------------------------------------
# optimise the deviance (-2*log[likelihood])

fit.gp <- function(theta.0, 
                   dat, 
                   method="Nelder-Mead", 
                   trace=0, 
                   theta.scale=NULL) {
  
  # -----------------------------------------------------------
  # fig.gp
  # Inputs: 
  #   theta  - vector of (hyper-)parameters for ACV/PSD
  #   dat    - 3 column data frame containing
  #     t    - vector of n observation times
  #     y    - vector of n observations
  #     dy   - vector of n 'errors' on observations
  #   method - choice of method for optim()
  #   theta.scale - vector of rescaling values for the 
  #                 parameters these. optim() will fit
  #                 theta.0/theta.scale.
  #
  # Value:
  #  outp - list of parameters (par) and their errors (err)
  #
  # Description:
  #  Find the Maximum Likelihood Estimates (MLEs) for the
  #  parameters theta of the ACF/PSD model, given the data
  #  {x, y, dy}.
  #
  # History:
  #  16/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------
  
  # check arguments
  if (missing(theta.0)) {stop('** Missing theta.0 input.')}
  if (!all(is.finite(theta.0))) {stop('** Non-finite values in theta.')}
  if (missing(dat$y)) {stop('** Missing dat$y.')}
  if (is.null(theta.scale)) { theta.scale <- rep(1, length(theta.0)) }

  # no. parameters?
  n.parm <- length(theta.0)
  
  # compute all the time differences 
  #  tau[i,j] = |t[j] - t[i]|
  tau <- matrix.tau(x)
  
  # check the initial position
  loglike.0 <- loglike(theta.0, tau, dat)
  if (!is.finite(loglike.0)) {
    stop('Non-finite log likelihood for initial theta value.')
  }
  
  # perform the fitting  
  result <- optim(fn = loglike, 
                  par = theta.0, 
                  method = method,
                  tau = tau, 
                  dat = dat,
                  hessian=TRUE,
                  control=list(trace=trace, parscale=theta.scale))
  
  print( (result) )
  
  # test if Hessian is non-singular. If so, estimate errors using
  # covariance matrix. Otherwise, set errors = 0.
  if (class(try(solve(result$hessian),silent=T))=="matrix") {
    covar <- solve(result$hessian)
    err <- sqrt(diag(covar))
  } else {
    err <- array(0, dim=length(result$par))
  }
  
  for (i in 1:n.parm) {
    cat('-- Parameter ', i, ': ', signif(result$par[i], 4), ' +/- ', 
        signif(err[i], 3), fill=TRUE, sep='')
  }
  
  # return the parameter values and their errors
  outp <- list(par = result$par, err = err)
  return(outp)  
}

# -----------------------------------------------------------
# Predict the mean of the Gaussian Process 

predict.gp <- function(theta, dat, t.pr) {
  
  # -----------------------------------------------------------
  # predict.gp
  # Inputs: 
  #   theta - vector of (hyper-)parameters for ACV/PSD
  #   dat    - 3 column data frame containing
  #     t    - vector of n observation times
  #     y    - vector of n observations
  #     dy   - vector of n 'errors' on observations
  #   t.pr  - vector of m times at which to predict 
  #
  # Value:
  #  result - list containing
  #     t   - prediction times
  #     y   - mean of GP model at times t
  #    dy   - sqrt(variance) of GP model at times t
  #   cov   - full m*m covariance matrix
  #
  # Description:
  # Predict the expectation of the GP with covariance matrix specified by
  # (hyper-)parameters 'theta' at times t.pr based on observations y.obs and
  # times t.obs. Uses eqn 2.23 of Rasmussen & Williams (2006).
  # 
  # Computes E[y(t.pr)] given y(t.obs) and theta, and also compute covariances
  # for times t.pr C[i,j] = C(t.pr[i], t.pr[j]). This may be a large matrix, so
  # we return by default only the leading diagnoal, i.e. the variances at times
  # t.pr.
  #
  # Use the following matricies each defined as
  # K[t1[i],t2[j]] = ACF(|t2[j] - t1[i]|)
  # if we have 
  #  t.obs - times at observations
  #  t.pr  - times at predictions
  #  K     - ACF(t.obs, t.obs)
  #  K.ik  - ACF(t.obs, t.pr)
  #  K.ki  - ACF(t.pr, t.obs)
  #  K.kk  - ACF(t.pr, t.pr)
  #
  # History:
  #  16/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------
  
  # check arguments
  if (missing(theta)) {stop('** Missing theta argument.')}
  if (!all(is.finite(theta))) {stop('** Non-finite values in theta.')}
  if (!exists('dat')) {stop('** Missing dat.')}
  if (!("y" %in% colnames(dat))) {stop('** Missing dat$y.')}

  # if times of predictions are not given, use times of the observations
  if (missing(t.pr)) { t.pr <- dat$t }

  # extract the mean - mu - from the theta vector
  mu <- theta[1]
  theta <- theta[-1]
  
  # compute model covariance matrix at observed delays
  tau.obs <- matrix.tau(dat$t, dat$t)
  K <- acv(theta, tau.obs)
  
  # compute matrix of covariances between observations and predictions
  tau.ki <- matrix.tau(t.pr, dat$t)
  tau.kk <- matrix.tau(t.pr, t.pr)
  K.ki <- acv(theta, tau.ki)
  K.kk <- acv(theta, tau.kk)
  
  # clean up memory
  rm(tau.obs, tau.ki, tau.kk)

  # eqn 2.20 of R&W - add the "error" term to the covariance matrix
  C <- K + dat$dy^2 * diag(1, NCOL(K))
  
  # compute the inverse covariance matrix
  C.inv <- solve(C)

  # eqn 2.23 of R&W - predict the mean value at prediction times 
  y.pr <- as.vector(K.ki %*% C.inv %*% (dat$y - mu)) + mu
  
  # eqn 2.24 of R&W - predict the covariances at all prediction times
  cov.pr <- K.kk - K.ki %*% C.inv %*% t(K.ki)
  
  # enforce for positive-definiteness (PD) and symmetry of the resulting covariance matrix.
  # This should be an m*m PD matrix but numerical errors may mean this is not exactly
  # true. We use the nearPD function from the Matrix package to find the nearest
  # PD matrix and use his.
  pd <- Matrix::nearPD(cov.pr)
  if (pd$converged == FALSE) {
    warning('** nearPD failed to converge.')
  }
  cov <- pd$mat
  rm(pd)
  
  # coerce into type 'matrix' for use with RMNORM function
  cov.pr <- matrix(cov.pr, nrow=length(t.pr))
  
  # Extract the diagonal elements from the covariance
  # matrix, i.e. the variances at times t.pr. Store this as a vector.
  # Use the absolute value to avoid any negative values creeping in 
  # due to numerical errors.
  dy.pr <- sqrt( as.vector( abs( diag(cov.pr) ) ) )
  
  # define the output product
  result <- list(t = t.pr, y = y.pr, dy = dy.pr, cov = cov.pr)
  return(result) 
}

# -----------------------------------------------------------
# generate a random GP realisation

sim.gp <- function(theta, dat, t.pr, N.sim=1, plot=FALSE) {

  # -----------------------------------------------------------
  # sim.gp
  # Inputs: 
  #   theta - vector of (hyper-)parameters for ACV/PSD
  #   dat    - 3 column data frame containing
  #     t    - vector of n observation times
  #     y    - vector of n observations
  #     dy   - vector of n 'errors' on observations
  #   t.pr  - vector of m times at which to simulate 
  #   N.sim - how many simulations to produce
  #   plot  - TRUE/FALSE - overlay a plot of the simulations?
  #
  # Value:
  #  y.out  - [n, N.sim] matrix, each column a simulation
  #
  # Description:
  # Simulate random realisations of a Gaussian Process (GP) with covariance
  # defined by parameters theta, at times t.pr, given data {t, y, dy}. This is
  # essentially eqn 2.22 of Rasmussen & Williams (2006):
  # 
  #   y.sim ~ N(mean = y.pr, cov = C)
  # 
  # Uses the function rmvnorm from the mvtnorm package to generate m-dimensional
  # Gaussian vectors. The mean values at times t.pr, y[t.pr] and covariance
  # matrix C for lags tau[i,j] = |t.pr[j] - t.pr[i]| are generated by the 
  # GP.PREDICT function.
  #
  # History:
  #  17/12/13 - First working version
  #
  # Simon Vaughan, University of Leicester
  # -----------------------------------------------------------

  # check arguments
  if (missing(theta)) {stop('** Missing theta argument.')}
  if (!all(is.finite(theta))) {stop('** Non-finite values in theta.')}
  if (!exists('dat')) {stop('** Missing dat.')}
  if (!("y" %in% colnames(dat))) {stop('** Missing dat$y.')}

  # if times of predictions are not given, use times of the observations
  if (missing(t.pr)) { t.pr <- dat$t }
  
  # length of input data, and simulated data
  n <- length(dat$y)
  m <- length(t.pr)

  # compute the mean and covariance matrix for the GP at times t.pr
  gp.out <- predict.gp(theta, dat, t.pr)
  
  # compute each time series, an m-vector drawn from the
  # multivariate (m-dimensional) Gaussian distribution.
  y.out <- mvtnorm::rmvnorm(n = N.sim, mean = gp.out$y, sigma = gp.out$cov)
  y.out <- t(y.out)
  
  # plot all the time series
  if (plot == TRUE) { 
    for (i in 1:N.sim) { lines(t.pr, y.out[,i], col = "grey60") }
  }
  
  # return the results
  return(y.out)
}

# -----------------------------------------------------------



